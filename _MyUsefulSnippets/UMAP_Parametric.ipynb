{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crazy-bubble",
   "metadata": {},
   "source": [
    "# UMAP Parametric\n",
    "#### Required packages: \n",
    "+ umap 0.5.0\n",
    "+ !pip install llvmlite>=0.34.0\n",
    "+ pynndescent # for the neural network embedding \n",
    "\n",
    "#### Main Resource : \n",
    "+ https://colab.research.google.com/drive/1lpdCy7HkC5TRI9LfUtIHBBW8oRO86Nvi?usp=sharing#scrollTo=NUEqcHlvXyKL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optimum-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it \n",
    "import sklearn\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly\n",
    "import plotly.graph_objs as pgo\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from pynndescent import NNDescent\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.utils import check_random_state\n",
    "from umap.umap_ import fuzzy_simplicial_set\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-complex",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prerequisite-startup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# the original function by Felix: \n",
    "# -------------------------------------\n",
    "\n",
    "'''\n",
    "Random Walk Operator with restart probability.\n",
    "Return Matrix.\n",
    "''' \n",
    "def rnd_walk_matrix2(A, r, a, num_nodes):\n",
    "\n",
    "    num = 1*num_nodes\n",
    "    n = num_nodes\n",
    "    factor = float((1-a)/n) # = 0 if alpha = 1.0 \n",
    "\n",
    "    E = np.multiply(factor,np.ones([n,n]))              # prepare 2nd scaling term\n",
    "    A_tele = np.multiply(a,A) + E  #     print(A_tele)\n",
    "    M = normalize(A_tele, norm='l1', axis=0)                                 # column wise normalized MArkov matrix\n",
    "\n",
    "    # mixture of Markov chains\n",
    "    del A_tele\n",
    "    del E\n",
    "\n",
    "    U = np.identity(n,dtype=int) \n",
    "    H = (1-r)*M\n",
    "    H1 = np.subtract(U,H)\n",
    "    del U\n",
    "    del M\n",
    "    del H    \n",
    "\n",
    "    W = r*np.linalg.inv(H1)   \n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# rwr modified (as used in this script) \n",
    "# -------------------------------------\n",
    "\n",
    "def rwr_matrix(A, r): \n",
    "    n = len(A) # take the whole network into account = all nodes are seed nodes\n",
    "    factor = 1 \n",
    "    a = 1 # maximum freedom to jump anytime to any node within the network\n",
    "    \n",
    "    E = np.multiply(factor,np.ones([n,n])) # Matrix with all 1 --> to \n",
    "    A_tele = np.multiply(a,A) + E  #     print(A_tele)\n",
    "    M = normalize(A_tele, norm='l1', axis=0)                                 # column wise normalized MArkov matrix\n",
    "\n",
    "    # mixture of Markov chains\n",
    "    del A_tele\n",
    "    del E\n",
    "\n",
    "    U = np.identity(n,dtype=int) \n",
    "    H = (1-r)*M\n",
    "    H1 = np.subtract(U,H)\n",
    "    del U\n",
    "    del M\n",
    "    del H    \n",
    "\n",
    "    W = r*np.linalg.inv(H1)   \n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# rwr simulated (just for testing, output = path from random walker)\n",
    "# -------------------------------------\n",
    "\n",
    "def random_walk_simple(start_v, number_of_steps):\n",
    "\n",
    "    for step in range(1, number_of_steps):\n",
    "\n",
    "        start_vertex = start_v # index of vertex to start from\n",
    "        visited_vertices = {} # Dictionary that associate nodes with the amount of times it was visited   \n",
    "\n",
    "        path = [start_vertex] # Store and print path  \n",
    "\n",
    "        counter = 0 # Restart the cycle\n",
    "        for counter in range(1, number_of_steps): \n",
    "            vertex_neighbors = [n for n in G.neighbors(start_vertex)] # get adjacent nodes\n",
    "            probability = [] # Set probability of going to a neighbour is uniform\n",
    "            probability = probability + [1./len(vertex_neighbors)] * len(vertex_neighbors)\n",
    "\n",
    "            start_vertex = np.random.choice(vertex_neighbors, p=probability) # Choose a vertex from the vertex neighborhood to start the next random walk\n",
    "\n",
    "            if start_vertex in visited_vertices: # Accumulate the amount of times each vertex is visited\n",
    "                visited_vertices[start_vertex] += 1\n",
    "            else:\n",
    "                visited_vertices[start_vertex] = 1\n",
    "\n",
    "            path.append(start_vertex) # Append to path\n",
    "\n",
    "        # mostvisited = sorted(visited_vertices, key = visited_vertices.get,reverse = True) # Organize the vertex list in most visited decrescent order\n",
    "        # print(\"Path: \", path)\n",
    "        # print(\"Most visited nodes: \", mostvisited[:10]) # Separate the top 10 most visited vertex\n",
    "        \n",
    "        return path\n",
    "\n",
    "    \n",
    "# -------------------------------------\n",
    "# plotting functions \n",
    "# -------------------------------------   \n",
    "\n",
    "def draw_node_degree(G, scalef):\n",
    "    #x = 20\n",
    "    #ring_frac = np.sqrt((x-1.)/x)\n",
    "    #ring_frac = (x-1.)/x\n",
    "\n",
    "    l_size = {}\n",
    "    for node in G.nodes():\n",
    "        k = nx.degree(G, node)\n",
    "        R = scalef * (1 + k**1.1) \n",
    "\n",
    "        l_size[node] = R\n",
    "        \n",
    "    return l_size\n",
    "\n",
    "\n",
    "def draw_node_degree_3D(G, scalef):\n",
    "    x = 3\n",
    "    ring_frac = (x-1.)/x\n",
    "\n",
    "    deg = dict(G.degree())\n",
    "    \n",
    "    d_size = {}\n",
    "    for i in G.nodes():\n",
    "        for k,v in deg.items():\n",
    "            if i == k:\n",
    "                R = scalef * (1+v**0.9)\n",
    "                r = ring_frac * R\n",
    "                d_size[i] = R\n",
    "    \n",
    "    return d_size \n",
    "\n",
    "\n",
    "def embed_umap_2D(Matrix, n_neighbors, spread, min_dist, metric='cosine'):\n",
    "    n_components = 2 # for 2D\n",
    "\n",
    "    U = umap.UMAP(\n",
    "        n_neighbors = n_neighbors,\n",
    "        spread = spread,\n",
    "        min_dist = min_dist,\n",
    "        n_components = n_components,\n",
    "        metric = metric)\n",
    "    embed = U.fit_transform(Matrix)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "def get_posG_2D(l_nodes, embed):\n",
    "    posG = {}\n",
    "    cc = 0\n",
    "    for entz in l_nodes:\n",
    "        # posG[str(entz)] = (embed[cc,0],embed[cc,1])\n",
    "        posG[entz] = (embed[cc,0],embed[cc,1])\n",
    "        cc += 1\n",
    "\n",
    "    return posG\n",
    "\n",
    "\n",
    "def get_posG_3D(l_genes, embed):\n",
    "    posG = {}\n",
    "    cc = 0\n",
    "    for entz in l_genes:\n",
    "        posG[entz] = (embed[cc,0],embed[cc,1],embed[cc,2])\n",
    "        cc += 1\n",
    "    \n",
    "    return posG\n",
    "\n",
    "\n",
    "def color_nodes_from_dict(G, d_to_be_coloured, color_method):\n",
    "\n",
    "    # Colouring\n",
    "    colour_groups = set(d_to_be_coloured.values())\n",
    "    colour_count = len(colour_groups)\n",
    "    pal = sns.color_palette('YlOrRd', colour_count)\n",
    "    palette = pal.as_hex()\n",
    "\n",
    "    d_colourgroups = {}\n",
    "    for n in colour_groups:\n",
    "        d_colourgroups[n] = [k for k in d_to_be_coloured.keys() if d_to_be_coloured[k] == n]\n",
    "        \n",
    "    d_colourgroups_sorted = {key:d_colourgroups[key] for key in sorted(d_colourgroups.keys())}\n",
    "\n",
    "    d_val_col = {}\n",
    "    for idx,val in enumerate(d_colourgroups_sorted):\n",
    "        for ix,v in enumerate(palette):\n",
    "            if idx == ix:\n",
    "                d_val_col[val] = v\n",
    "\n",
    "    d_node_colour = {}\n",
    "    for y in d_to_be_coloured.items(): # y[0] = node id, y[1] = val\n",
    "        for x in d_val_col.items(): # x[0] = val, x[1] = (col,col,col)\n",
    "            if x[0] == y[1]:\n",
    "                d_node_colour[y[0]]=x[1]\n",
    "\n",
    "    # SORT dict based on G.nodes\n",
    "    d_node_colour_sorted = dict([(key, d_node_colour[key]) for key in G.nodes()])\n",
    "    l_col = list(d_node_colour_sorted.values())\n",
    "    colours = l_col\n",
    "\n",
    "    return colours\n",
    "\n",
    "def color_clusters_from_dict(G, d_to_be_coloured, color_method):\n",
    "\n",
    "    # Colouring\n",
    "    colour_groups = set(d_to_be_coloured.values())\n",
    "    colour_count = len(colour_groups)\n",
    "    pal = sns.color_palette('Spectral', colour_count)\n",
    "    palette = pal.as_hex()\n",
    "\n",
    "    d_colourgroups = {}\n",
    "    for n in colour_groups:\n",
    "        d_colourgroups[n] = [k for k in d_to_be_coloured.keys() if d_to_be_coloured[k] == n]\n",
    "        \n",
    "    d_colourgroups_sorted = {key:d_colourgroups[key] for key in sorted(d_colourgroups.keys())}\n",
    "\n",
    "    d_val_col = {}\n",
    "    for idx,val in enumerate(d_colourgroups_sorted):\n",
    "        for ix,v in enumerate(palette):\n",
    "            if idx == ix:\n",
    "                d_val_col[val] = v\n",
    "\n",
    "    d_node_colour = {}\n",
    "    for y in d_to_be_coloured.items(): # y[0] = node id, y[1] = val\n",
    "        for x in d_val_col.items(): # x[0] = val, x[1] = (col,col,col)\n",
    "            if x[0] == y[1]:\n",
    "                d_node_colour[y[0]]=x[1]\n",
    "\n",
    "    # SORT dict based on G.nodes\n",
    "    d_node_colour_sorted = dict([(key, d_node_colour[key]) for key in G.nodes()])\n",
    "    l_col = list(d_node_colour_sorted.values())\n",
    "    colours = l_col\n",
    "    \n",
    "    return colours\n",
    "\n",
    "\n",
    "def embed_umap_3D(Matrix, n_neighbors, spread, min_dist, metric='cosine'):\n",
    "    n_components = 3 # for 3D\n",
    "\n",
    "    U_3d = umap.UMAP(\n",
    "        n_neighbors = n_neighbors,\n",
    "        spread = spread,\n",
    "        min_dist = min_dist,\n",
    "        n_components = n_components,\n",
    "        metric = metric)\n",
    "    embed = U_3d.fit_transform(Matrix)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "def get_posG_3D(l_genes, embed):\n",
    "    posG = {}\n",
    "    cc = 0\n",
    "    for entz in l_genes:\n",
    "        posG[entz] = (embed[cc,0],embed[cc,1],embed[cc,2])\n",
    "        cc += 1\n",
    "    \n",
    "    return posG\n",
    "\n",
    "\n",
    "def get_trace_nodes_3D(posG, info_list, color_list, size):\n",
    "\n",
    "    key_list=list(posG.keys())\n",
    "    trace = pgo.Scatter3d(x=[posG[key_list[i]][0] for i in range(len(key_list))],\n",
    "                           y=[posG[key_list[i]][1] for i in range(len(key_list))],\n",
    "                           z=[posG[key_list[i]][2] for i in range(len(key_list))],\n",
    "                           mode = 'markers',\n",
    "                           text = info_list,\n",
    "                           hoverinfo = 'text',\n",
    "                           #textposition='middle center',\n",
    "                           marker = dict(\n",
    "                color = color_list,\n",
    "                size = size,\n",
    "                symbol = 'circle',\n",
    "                line = dict(width = 1.0,\n",
    "                        color = color_list)\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    return trace\n",
    "\n",
    "\n",
    "def get_trace_edges_3D(G, posG, color_list, opac = 0.2):\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_z = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0, z0 = posG[edge[0]]\n",
    "        x1, y1, z1 = posG[edge[1]]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        edge_z.append(z0)\n",
    "        edge_z.append(z1)\n",
    "        edge_z.append(None)\n",
    "\n",
    "    trace_edges = pgo.Scatter3d(\n",
    "                        x = edge_x, \n",
    "                        y = edge_y, \n",
    "                        z = edge_z,\n",
    "                        mode = 'lines', hoverinfo='none',\n",
    "                        line = dict(width = 0.5, color = color_list),\n",
    "                        opacity = opac\n",
    "                )\n",
    "    \n",
    "    return trace_edges\n",
    "\n",
    "\n",
    "def color_nodes_same(G, list_color_nodes, color):\n",
    "\n",
    "    d_col = {}\n",
    "    for node in list_color_nodes:\n",
    "            d_col[node] = color\n",
    "      \n",
    "    return d_col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "successful-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH input \n",
    "organism = 'Human'\n",
    "\n",
    "G = nx.read_edgelist('input/ppi_elist.txt',data=False)\n",
    "# d_ent_sym, d_sym_ent = genent2sym()\n",
    "\n",
    "d_gene_do = pickle.load( open( \"input/d_gene_do.pkl\", \"rb\" ) )\n",
    "d_do_genes = pickle.load( open( \"input/d_do_genes.pkl\", \"rb\" ) )\n",
    "d_do_names = pickle.load( open( \"input/DO_names.pkl\", \"rb\" ) )\n",
    "d_names_do = {y:x for x,y in d_do_names.items()}\n",
    "\n",
    "df_gene_sym = pd.read_csv('output_csv/DF_gene_symbol_Human.csv', index_col=0)\n",
    "l_features = list((df_gene_sym.to_dict()).values())\n",
    "\n",
    "posG_entrez = []\n",
    "for k in G.nodes():\n",
    "    posG_entrez.append(k)\n",
    "    \n",
    "    \n",
    "features_MF = pd.read_csv('output_csv/Features_GO_MolFunc_Dataframe_Human.csv', index_col=0)\n",
    "feat_genes = list(features_MF.index)\n",
    "    \n",
    "    \n",
    "A_graph = nx.adjacency_matrix(G)\n",
    "A = A_graph.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "harmful-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.8 \n",
    "\n",
    "struct_rwr_matrix = rwr_matrix(A,r)\n",
    "\n",
    "df_struct = pd.DataFrame(struct_rwr_matrix, columns = list(G.nodes()), index=list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT FOR EMBEDDING â‰  Distance Matrix, but instead FEATURE MATRIX\n",
    "\n",
    "#DM = df_struct\n",
    "#feature='structural'\n",
    "\n",
    "DM = df_func\n",
    "feature='functional'\n",
    "\n",
    "# set gene list (= G.nodes())\n",
    "genes = []\n",
    "for i in DM.index:\n",
    "    genes.append(str(i))\n",
    "\n",
    "genes_rest = [] \n",
    "for g in G.nodes():\n",
    "    if g not in genes:\n",
    "        genes_rest.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-special",
   "metadata": {},
   "source": [
    "# Parametric UMAP 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import ParametricUMAP\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "from umap.umap_ import fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features_MF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(15165,3744,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-boring",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-internet",
   "metadata": {},
   "source": [
    "## Part 1 : Building the Graph \n",
    "#### Build nearest neighbor's graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "olive-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "#X = train_images.reshape((60000, 28, 28, 1))\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in random projection forest\n",
    "n_trees = 5 + int(round((X.shape[0]) ** 0.5 / 20.0))\n",
    "\n",
    "# max number of nearest neighbor iters to perform\n",
    "n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "\n",
    "# distance metric\n",
    "metric=\"euclidean\"\n",
    "\n",
    "# number of neighbors for computing k-neighbor graph\n",
    "n_neighbors = 10\n",
    "\n",
    "# get nearest neighbors\n",
    "nnd = NNDescent(\n",
    "    X.reshape((len(X), np.product(np.shape(X)[1:]))),\n",
    "    n_neighbors=n_neighbors,\n",
    "    metric=metric,\n",
    "    n_trees=n_trees,\n",
    "    n_iters=n_iters,\n",
    "    max_candidates=60,\n",
    "    verbose=True\n",
    ")\n",
    "# get indices and distances\n",
    "knn_indices, knn_dists = nnd.neighbor_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neighbors and distances for each point in the dataset\n",
    "np.shape(knn_indices), np.shape(knn_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-bride",
   "metadata": {},
   "source": [
    "+ build fuzzy simplicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from umap.umap_ import fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices and distances\n",
    "knn_indices, knn_dists = nnd.neighbor_graph\n",
    "random_state = check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fuzzy_simplicial_set\n",
    "umap_graph, sigmas, rhos = fuzzy_simplicial_set(\n",
    "    X = X,\n",
    "    n_neighbors = n_neighbors,\n",
    "    metric = metric,\n",
    "    random_state = random_state,\n",
    "    knn_indices= knn_indices,\n",
    "    knn_dists = knn_dists,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-catholic",
   "metadata": {},
   "source": [
    "+ embedding the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2 # number of latent dimensions\n",
    "dims = X.shape[1:]\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\", padding=\"same\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-medium",
   "metadata": {},
   "source": [
    "+ create batch iterator > iteration of batches of edges and nearste-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import construct_edge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs isused to compute epochs_per_sample, which, in non-parametric UMAP, \n",
    "# is the total number of epochs to optimize embeddings over. The computed value \n",
    "# epochs_per_sample, is the number of epochs  each edge is optimized over \n",
    "# (higher probability = more epochs).\n",
    "n_epochs = 200 \n",
    "\n",
    "batch_size = 1000 # iterate over batches of 1000 edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensorflow dataset of edges \n",
    "(\n",
    "    edge_dataset,\n",
    "    batch_size,\n",
    "    n_edges,\n",
    "    head,\n",
    "    tail,\n",
    "    edge_weight,\n",
    ") = construct_edge_dataset(\n",
    "    X,\n",
    "    umap_graph,\n",
    "    n_epochs,\n",
    "    batch_size,\n",
    "    parametric_embedding = True,\n",
    "    parametric_reconstruction = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_edge_to_x, sample_edge_from_x), _ =  next(iter(edge_dataset))\n",
    "sample_edge_to_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-phoenix",
   "metadata": {},
   "source": [
    "+ compute loss over a batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import umap_loss\n",
    "from umap.umap_ import find_ab_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = 0.1 # controls how tightly UMAP is allowed to pack points together (0 is more)\n",
    "_a, _b = find_ab_params(1.0, min_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-citizenship",
   "metadata": {},
   "source": [
    "+ define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_rate = 5 # how many negative samples to train on per edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab z for the edge\n",
    "embedding_to = encoder.predict(sample_edge_to_x)\n",
    "embedding_from = encoder.predict(sample_edge_from_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get negative samples by randomly shuffling the batch\n",
    "embedding_neg_to = tf.repeat(embedding_to, negative_sample_rate, axis=0)\n",
    "repeat_neg = tf.repeat(embedding_from, negative_sample_rate, axis=0)\n",
    "embedding_neg_from = tf.gather(\n",
    "    repeat_neg, tf.random.shuffle(tf.range(tf.shape(repeat_neg)[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distances between samples (and negative samples)\n",
    "distance_embedding = tf.concat(\n",
    "    [\n",
    "        tf.norm(embedding_to - embedding_from, axis=1),\n",
    "        tf.norm(embedding_neg_to - embedding_neg_from, axis=1),\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_distance_to_probability(distances, a=1.0, b=1.0):\n",
    "    return 1.0 / (1.0 + a * distances ** (2 * b))\n",
    "\n",
    "# convert probabilities to distances\n",
    "probabilities_distance = convert_distance_to_probability(\n",
    "    distance_embedding, _a, _b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set true probabilities based on negative sampling\n",
    "probabilities_graph = tf.concat(\n",
    "    [tf.ones(batch_size), tf.zeros(batch_size * negative_sample_rate)], axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(\n",
    "    probabilities_graph, probabilities_distance, EPS=1e-4, repulsion_strength=1.0\n",
    "):\n",
    "    # cross entropy\n",
    "    attraction_term = -probabilities_graph * tf.math.log(\n",
    "        tf.clip_by_value(probabilities_distance, EPS, 1.0)\n",
    "    )\n",
    "    repellant_term = (\n",
    "        -(1.0 - probabilities_graph)\n",
    "        * tf.math.log(tf.clip_by_value(1.0 - probabilities_distance, EPS, 1.0))\n",
    "        * repulsion_strength\n",
    "    )\n",
    "\n",
    "    # balance the expected losses between atrraction and repel\n",
    "    CE = attraction_term + repellant_term\n",
    "    return attraction_term, repellant_term, CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross entropy\n",
    "(attraction_loss, repellant_loss, ce_loss) = compute_cross_entropy(\n",
    "    probabilities_graph,\n",
    "    probabilities_distance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import umap_loss \n",
    "\n",
    "umap_loss_fn = umap_loss(\n",
    "    batch_size,\n",
    "    negative_sample_rate,\n",
    "    _a,\n",
    "    _b,\n",
    "    edge_weight,\n",
    "    parametric_embedding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-muscle",
   "metadata": {},
   "source": [
    "+ define and compile the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inputs\n",
    "to_x = tf.keras.layers.Input(shape=dims, name=\"to_x\")\n",
    "from_x = tf.keras.layers.Input(shape=dims, name=\"from_x\")\n",
    "inputs = [to_x, from_x]\n",
    "\n",
    "# parametric embedding\n",
    "embedding_to = encoder(to_x)\n",
    "embedding_from = encoder(from_x)\n",
    "\n",
    "# concatenate to/from projections for loss computation\n",
    "embedding_to_from = tf.concat([embedding_to, embedding_from], axis=1)\n",
    "embedding_to_from = tf.keras.layers.Lambda(lambda x: x, name=\"umap\")(\n",
    "    embedding_to_from\n",
    ")\n",
    "outputs = {'umap': embedding_to_from}\n",
    "\n",
    "# create model\n",
    "parametric_model = tf.keras.Model(inputs=inputs, outputs=outputs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametric_model.compile(\n",
    "    optimizer=optimizer, loss=umap_loss_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-gauge",
   "metadata": {},
   "source": [
    "+ fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(\n",
    "    n_edges / batch_size / 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding\n",
    "history = parametric_model.fit(\n",
    "    edge_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    max_queue_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-differential",
   "metadata": {},
   "source": [
    "+ take look at final embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data\n",
    "z = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(10, 8))\n",
    "sc = ax.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "    c=train_labels,\n",
    "    cmap=\"tab10\",\n",
    "    s=0.1,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,\n",
    ")\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"UMAP embeddings\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-profile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-brook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-baker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-senate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exclusive-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 26 17:16:57 2021 Building RP forest with 17 trees\n",
      "Tue Jan 26 17:17:01 2021 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\t 4  /  16\n",
      "\t 5  /  16\n",
      "\tStopping threshold met -- exiting after 5 iterations\n"
     ]
    }
   ],
   "source": [
    "# number of trees in random projection forest\n",
    "n_trees = 5 + int(round((X.shape[0]) ** 0.5 / 20.0))\n",
    "# max number of nearest neighbor iters to perform\n",
    "n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "# distance metric\n",
    "metric=\"euclidean\"\n",
    "# number of neighbors for computing k-neighbor graph\n",
    "n_neighbors = 10\n",
    "\n",
    "# get nearest neighbors\n",
    "nnd = NNDescent(\n",
    "    X.reshape((len(X), np.product(np.shape(X)[1:]))),\n",
    "    n_neighbors=n_neighbors,\n",
    "    metric=metric,\n",
    "    n_trees=n_trees,\n",
    "    n_iters=n_iters,\n",
    "    max_candidates=60,\n",
    "    verbose=True\n",
    ")\n",
    "# get indices and distances\n",
    "knn_indices, knn_dists = nnd.neighbor_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corrected-channel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest neighbors and distances for each point in the dataset\n",
    "np.shape(knn_indices), np.shape(knn_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-discharge",
   "metadata": {},
   "source": [
    "#### Build fuzzy simplicial complex\n",
    "+ The fuzzy_simplicial_set function takes the nearest neighbor graph and computes a graph of the probabilities of an edge exists between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "close-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices and distances\n",
    "knn_indices, knn_dists = nnd.neighbor_graph\n",
    "random_state = check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabulous-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fuzzy_simplicial_set\n",
    "umap_graph, sigmas, rhos = fuzzy_simplicial_set(\n",
    "    X = X,\n",
    "    n_neighbors = n_neighbors,\n",
    "    metric = metric,\n",
    "    random_state = random_state,\n",
    "    knn_indices= knn_indices,\n",
    "    knn_dists = knn_dists,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "honey-night",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<60000x60000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 803674 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-black",
   "metadata": {},
   "source": [
    "## VISUALIZATION STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node, Edge colors\n",
    "\n",
    "edge_width = 0.1\n",
    "edge_color = 'lightgrey'\n",
    "\n",
    "edge_col = 'lightgrey'\n",
    "edge_colordark = 'dimgrey'\n",
    "opacity_edges = 0.5\n",
    "\n",
    "opacity_nodes = 1.0\n",
    "node_edge_col = None\n",
    "\n",
    "\n",
    "# Node sizes \n",
    "\n",
    "#size = 10.0\n",
    "#size3d = 5.0\n",
    "\n",
    "# if node size reflects degree : \n",
    "\n",
    "scalef= 0.25\n",
    "size = list(draw_node_degree(G, scalef).values())\n",
    "\n",
    "scalef= 0.05\n",
    "size3d = list(draw_node_degree_3D(G, scalef).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality metrics \n",
    "color_method = 'clos'\n",
    "\n",
    "df_centralities = pd.read_csv('output_csv/Features_centralities_Dataframe_'+organism+'.csv', index_col=0)\n",
    "\n",
    "d_deghubs = dict(zip(G.nodes(),df_centralities['degs']))\n",
    "d_clos = dict(zip(G.nodes(), df_centralities['clos']))\n",
    "d_betw = dict(zip(G.nodes(), df_centralities['betw']))\n",
    "d_eigen = dict(zip(G.nodes(), df_centralities['eigen']))\n",
    "\n",
    "# uncomment to set colours\n",
    "colours = color_nodes_from_dict(G, d_clos, color_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_umap_parametric_2D(Matrix, n_neighbors, spread, min_dist, metric='cosine'):\n",
    "    n_components = 2 # for 2D\n",
    "\n",
    "    U = ParametricUMAP(\n",
    "        n_neighbors = n_neighbors,\n",
    "        spread = spread,\n",
    "        min_dist = min_dist,\n",
    "        n_components = n_components,\n",
    "        metric = metric)\n",
    "    embed = U.fit_transform(Matrix)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "umapparam_2D = embed_umap_parametric_2D(DM, n_neighbors, spread, min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "posG_umapparam = get_posG_2D(genes, umapparam_2D)\n",
    "df_posGparam = pd.DataFrame(posG_umapparam).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-sender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-payday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-collect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-tours",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
